{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading datasets\n",
    "class NpyImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    General dataset for loading .npy image files from a directory structure.\n",
    "    Assumes that each class has its own subdirectory.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (str): Root directory containing subdirectories for each class.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # List all subdirectories (each is a class)\n",
    "        class_dirs = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "        # Create a mapping from class name to integer index\n",
    "        self.class_to_idx = {class_name: i for i, class_name in enumerate(sorted(class_dirs))}\n",
    "        \n",
    "        # Collect all .npy files and assign labels based on folder\n",
    "        for class_name in class_dirs:\n",
    "            class_path = os.path.join(data_dir, class_name)\n",
    "            class_idx = self.class_to_idx[class_name]\n",
    "            for file_path in glob.glob(os.path.join(class_path, '*.npy')):\n",
    "                self.image_paths.append(file_path)\n",
    "                self.labels.append(class_idx)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image from .npy file\n",
    "        image = np.load(self.image_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "        image = torch.from_numpy(image).float()\n",
    "        # If image is 2D (H x W), add a channel dimension.\n",
    "        if image.dim() == 2:\n",
    "            image = image.unsqueeze(0)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "class FilteredNpyImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that filters the samples from NpyImageDataset to only include one target class.\n",
    "    This is used for pretraining the MAE on the 'no_sub' samples.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, target_class, transform=None):\n",
    "        base_dataset = NpyImageDataset(data_dir, transform=transform)\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_to_idx = base_dataset.class_to_idx\n",
    "        target_idx = self.class_to_idx[target_class]\n",
    "        # Filter for only those samples whose label matches target_idx\n",
    "        for path, label in zip(base_dataset.image_paths, base_dataset.labels):\n",
    "            if label == target_idx:\n",
    "                self.image_paths.append(path)\n",
    "                self.labels.append(label)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = np.load(self.image_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "        image = torch.from_numpy(image).float()\n",
    "        if image.dim() == 2:\n",
    "            image = image.unsqueeze(0)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "class NpySuperResolutionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for the super-resolution task.\n",
    "    Expects that low-resolution (LR) and high-resolution (HR) .npy images are stored\n",
    "    in separate directories but with matching filenames.\n",
    "    \"\"\"\n",
    "    def __init__(self, lr_dir, hr_dir, transform_lr=None, transform_hr=None):\n",
    "        self.lr_paths = sorted(glob.glob(os.path.join(lr_dir, '*.npy')))\n",
    "        self.hr_paths = sorted(glob.glob(os.path.join(hr_dir, '*.npy')))\n",
    "        assert len(self.lr_paths) == len(self.hr_paths), \"Mismatch in number of LR and HR files\"\n",
    "        self.transform_lr = transform_lr\n",
    "        self.transform_hr = transform_hr\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr = np.load(self.lr_paths[idx])\n",
    "        hr = np.load(self.hr_paths[idx])\n",
    "        lr = torch.from_numpy(lr).float()\n",
    "        hr = torch.from_numpy(hr).float()\n",
    "        if lr.dim() == 2:\n",
    "            lr = lr.unsqueeze(0)\n",
    "        if hr.dim() == 2:\n",
    "            hr = hr.unsqueeze(0)\n",
    "        if self.transform_lr:\n",
    "            lr = self.transform_lr(lr)\n",
    "        if self.transform_hr:\n",
    "            hr = self.transform_hr(hr)\n",
    "        return lr, hr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_patch_mask(img, mask_ratio=0.75, patch_size=16):\n",
    "    \"\"\"\n",
    "    Applies a patch-level mask to an image.\n",
    "    Divides the image (C, H, W) into patches of size patch_size x patch_size,\n",
    "    randomly masks a fraction (mask_ratio) of the patches (sets them to 0),\n",
    "    and returns both the masked image and the binary mask.\n",
    "    \"\"\"\n",
    "    C, H, W = img.shape\n",
    "    # Ensure H and W are divisible by patch_size\n",
    "    assert H % patch_size == 0 and W % patch_size == 0, \"Image dimensions must be divisible by patch_size\"\n",
    "    grid_h, grid_w = H // patch_size, W // patch_size\n",
    "    # Create a mask for each patch (True if the patch is to be masked)\n",
    "    patch_mask = (torch.rand(grid_h, grid_w) < mask_ratio)\n",
    "    # Expand patch mask to pixel resolution\n",
    "    mask = patch_mask.unsqueeze(0).repeat(C, 1, 1)\n",
    "    mask = mask.repeat_interleave(patch_size, dim=1).repeat_interleave(patch_size, dim=2)\n",
    "    # Create masked image: set masked pixels to 0\n",
    "    img_masked = img.clone()\n",
    "    img_masked[mask] = 0.0\n",
    "    return img_masked, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Masked Autoencoder model.\n",
    "    The encoder is a small convolutional network and the decoder upsamples back to the original resolution.\n",
    "    The model is trained to reconstruct only the masked portions of the input.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, latent_dim=128):\n",
    "        super(MAE, self).__init__()\n",
    "        # Encoder: 3 conv layers with downsampling\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, stride=2, padding=1),   # -> (32, H/2, W/2)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),              # -> (64, H/4, W/4)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, latent_dim, kernel_size=3, stride=2, padding=1),        # -> (latent_dim, H/8, W/8)\n",
    "            nn.BatchNorm2d(latent_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Decoder: 3 transposed conv layers for upsampling\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 64, kernel_size=3, stride=2, padding=1, output_padding=1),  # -> (64, H/4, W/4)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=0),           # -> (32, H/2, W/2)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, in_channels, kernel_size=3, stride=2, padding=1, output_padding=1),    # -> (in_channels, H, W)\n",
    "            nn.Sigmoid()  # To constrain outputs to [0, 1]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        recon = self.decoder(latent)\n",
    "        return recon, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    classification model with standard blocks.\n",
    "    Uses dropout and batch normalization for better generalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, latent_dim=128, num_classes=3):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        #using the pretrained encoder\n",
    "        self.encoder = encoder  \n",
    "        \n",
    "        #adding a few additional layers to adapt the encoder features\n",
    "        self.adapt_layers = nn.Sequential(\n",
    "            nn.Conv2d(latent_dim, latent_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(latent_dim, latent_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        #global average pooling followed by classifier head\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        #classifier head with dropout for regularization\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),  #dropout for regularization\n",
    "            nn.Linear(256, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        features = self.adapt_layers(features)\n",
    "        #global average pooling\n",
    "        pooled = self.avgpool(features)\n",
    "        #classification\n",
    "        output = self.classifier(pooled)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperResolutionModel(nn.Module):\n",
    "    def __init__(self, encoder, latent_dim=128, in_channels=1, lr_size=(80, 80), hr_size=(150, 150)):\n",
    "        super(SuperResolutionModel, self).__init__()\n",
    "        self.encoder = encoder  #using pretrained encoder\n",
    "        self.lr_size = lr_size\n",
    "        self.hr_size = hr_size\n",
    "        \n",
    "        self.scale_factor = hr_size[0] / (lr_size[0] // 8)  #enoder downsamples by 8x\n",
    "        \n",
    "        #feature processing\n",
    "        self.process_features = nn.Sequential(\n",
    "            nn.Conv2d(latent_dim, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        #progressive upsampling to target size\n",
    "        self.upsampler = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 2x upscale\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # 2x upscale\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),    # 2x upscale\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        #final layer to adjust channel count and apply final adjustments\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(32, in_channels, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        processed = self.process_features(features)\n",
    "        \n",
    "        \n",
    "        #upsample progressively\n",
    "        upsampled = self.upsampler(processed)\n",
    "\n",
    "\n",
    "        out = self.final(upsampled)\n",
    "        \n",
    "        #resize to target HR size if needed\n",
    "        if out.shape[2:] != self.hr_size:\n",
    "            out = F.interpolate(out, size=self.hr_size, mode='bilinear', align_corners=False)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mae(model, dataloader, num_epochs=20, device='cuda', mask_ratio=0.75, patch_size=16):\n",
    "    \"\"\"\n",
    "    Train the MAE model using masked reconstruction loss (MSE computed only on masked pixels).\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    # We use MSELoss but will manually weight the loss on masked regions.\n",
    "    mse_loss = nn.MSELoss(reduction='none')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, _ in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images = images.to(device)\n",
    "            # Apply patch masking to each image in the batch\n",
    "            masked_images = []\n",
    "            masks = []\n",
    "            for img in images:\n",
    "                img_masked, mask = apply_patch_mask(img, mask_ratio, patch_size)\n",
    "                masked_images.append(img_masked)\n",
    "                masks.append(mask)\n",
    "            masked_images = torch.stack(masked_images).to(device)\n",
    "            masks = torch.stack(masks).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            recon, _ = model(masked_images)\n",
    "            # Resize reconstruction to match original image size if dimensions don't match\n",
    "            if recon.shape != images.shape:\n",
    "                recon = F.interpolate(recon, size=(images.shape[2], images.shape[3]), mode='bilinear', align_corners=False)\n",
    "            # Compute loss only on masked pixels\n",
    "            loss_map = mse_loss(recon, images)\n",
    "            loss = (loss_map * masks).sum() / masks.sum()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        print(f\"[MAE Pretraining] Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.6f}\")\n",
    "        \n",
    "    # Visualize 5 random examples: original, masked, and reconstructed images\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, _ in dataloader:\n",
    "            # Select 5 random images from the batch\n",
    "            indices = random.sample(range(images.shape[0]), 5)\n",
    "            originals, masked, reconstructions = [], [], []\n",
    "            for i in indices:\n",
    "                img = images[i].to(device)\n",
    "                img_masked, _ = apply_patch_mask(img, mask_ratio, patch_size)\n",
    "                output, _ = model(img_masked.unsqueeze(0))\n",
    "                # Resize if necessary to match original image dimensions\n",
    "                if output.shape != img.unsqueeze(0).shape:\n",
    "                    output = F.interpolate(output, size=(img.shape[1], img.shape[2]), mode='bilinear', align_corners=False)\n",
    "                originals.append(img.cpu())\n",
    "                masked.append(img_masked.cpu())\n",
    "                reconstructions.append(output.squeeze(0).cpu())\n",
    "            break\n",
    "\n",
    "    fig, axes = plt.subplots(5, 3, figsize=(12, 20))\n",
    "    for i in range(5):\n",
    "        axes[i, 0].imshow(originals[i].squeeze(), cmap='gray')\n",
    "        axes[i, 0].set_title(\"Original\")\n",
    "        axes[i, 0].axis('off')\n",
    "        axes[i, 1].imshow(masked[i].squeeze(), cmap='gray')\n",
    "        axes[i, 1].set_title(\"Masked\")\n",
    "        axes[i, 1].axis('off')\n",
    "        axes[i, 2].imshow(reconstructions[i].squeeze(), cmap='gray')\n",
    "        axes[i, 2].set_title(\"Reconstructed\")\n",
    "        axes[i, 2].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # save image\n",
    "    plt.savefig('reconstruction_results.png')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, train_loader, val_loader, num_epochs=10, device='cuda'):\n",
    "    \"\"\"\n",
    "    Improved training function for the classifier with learning rate scheduling,\n",
    "    early stopping, and model checkpointing.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Use SGD with momentum instead of Adam for better convergence\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "    \n",
    "    # Learning rate scheduler - reduce LR when plateau\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "    \n",
    "    best_auc = 0.0\n",
    "    best_model_state = None\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            \n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_acc = 100.0 * correct / total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                # For AUC calculation\n",
    "                probs = torch.softmax(outputs, dim=1)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_acc = 100.0 * val_correct / val_total\n",
    "        \n",
    "        # Convert ground truth to one-hot encoding for roc_auc_score\n",
    "        num_classes = len(set(all_labels))\n",
    "        one_hot_labels = np.eye(num_classes)[np.array(all_labels)]\n",
    "        try:\n",
    "            auc_score = roc_auc_score(one_hot_labels, np.array(all_probs), average='macro')\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error calculating AUC: {str(e)}\")\n",
    "            auc_score = 0.0\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"[Classifier] Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, \"\n",
    "              f\"ROC AUC: {auc_score:.4f}\")\n",
    "        \n",
    "        # Update learning rate based on AUC\n",
    "        scheduler.step(auc_score)\n",
    "        \n",
    "        # Check if this is the best model (based on AUC)\n",
    "        if auc_score > best_auc:\n",
    "            best_auc = auc_score\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Load the best model weights\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"Loaded best model with AUC: {best_auc:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_super_resolution(model, dataloader, num_epochs=10, device='cuda'):\n",
    "    \"\"\"\n",
    "    Improved training function for super-resolution with L1 loss component\n",
    "    and proper handling of output size.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    # Combine MSE and L1 loss for better results\n",
    "    mse_criterion = nn.MSELoss()\n",
    "    l1_criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=2e-4, betas=(0.9, 0.999))\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_mse_loss = 0.0\n",
    "        running_l1_loss = 0.0\n",
    "        running_total_loss = 0.0\n",
    "        \n",
    "        for lr_imgs, hr_imgs in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            lr_imgs, hr_imgs = lr_imgs.to(device), hr_imgs.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(lr_imgs)\n",
    "            \n",
    "            # Ensure output size matches target size\n",
    "            if outputs.shape != hr_imgs.shape:\n",
    "                outputs = F.interpolate(outputs, size=(hr_imgs.shape[2], hr_imgs.shape[3]), \n",
    "                                        mode='bilinear', align_corners=False)\n",
    "                \n",
    "            # Calculate losses\n",
    "            mse_loss = mse_criterion(outputs, hr_imgs)\n",
    "            l1_loss = l1_criterion(outputs, hr_imgs)\n",
    "            # Combined loss (MSE with L1 regularization)\n",
    "            loss = mse_loss + 0.5 * l1_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            running_mse_loss += mse_loss.item() * lr_imgs.size(0)\n",
    "            running_l1_loss += l1_loss.item() * lr_imgs.size(0)\n",
    "            running_total_loss += loss.item() * lr_imgs.size(0)\n",
    "            \n",
    "        # Compute epoch losses\n",
    "        epoch_mse_loss = running_mse_loss / len(dataloader.dataset)\n",
    "        epoch_l1_loss = running_l1_loss / len(dataloader.dataset)\n",
    "        epoch_total_loss = running_total_loss / len(dataloader.dataset)\n",
    "        \n",
    "        # PSNR calculation (assuming images normalized in [0,1])\n",
    "        psnr = 10 * math.log10(1.0 / (epoch_mse_loss + 1e-8))\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"[Super-Resolution] Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"MSE Loss: {epoch_mse_loss:.6f}, L1 Loss: {epoch_l1_loss:.6f}, \"\n",
    "              f\"Total Loss: {epoch_total_loss:.6f}, PSNR: {psnr:.2f} dB\")\n",
    "    \n",
    "    # Visualize a few examples\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for lr_imgs, hr_imgs in dataloader:\n",
    "            lr_imgs, hr_imgs = lr_imgs.to(device), hr_imgs.to(device)\n",
    "            sr_imgs = model(lr_imgs)\n",
    "            \n",
    "            # Ensure correct size for visualization\n",
    "            if sr_imgs.shape != hr_imgs.shape:\n",
    "                sr_imgs = F.interpolate(sr_imgs, size=(hr_imgs.shape[2], hr_imgs.shape[3]), \n",
    "                                        mode='bilinear', align_corners=False)\n",
    "            \n",
    "            # Pick a few samples to visualize\n",
    "            n_samples = min(3, lr_imgs.size(0))\n",
    "            \n",
    "            fig, axes = plt.subplots(n_samples, 3, figsize=(15, 5*n_samples))\n",
    "            for i in range(n_samples):\n",
    "                # Low-resolution input\n",
    "                axes[i, 0].imshow(lr_imgs[i].cpu().squeeze(0), cmap='gray')\n",
    "                axes[i, 0].set_title(\"Low Resolution\")\n",
    "                axes[i, 0].axis('off')\n",
    "                \n",
    "                # Super-resolution output\n",
    "                axes[i, 1].imshow(sr_imgs[i].cpu().squeeze(0), cmap='gray')\n",
    "                axes[i, 1].set_title(\"Super Resolution\")\n",
    "                axes[i, 1].axis('off')\n",
    "                \n",
    "                # High-resolution ground truth\n",
    "                axes[i, 2].imshow(hr_imgs[i].cpu().squeeze(0), cmap='gray')\n",
    "                axes[i, 2].set_title(\"High Resolution\")\n",
    "                axes[i, 2].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.savefig('super_resolution_results.png')\n",
    "            break\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "#any necessary transforms (here we assume images are already normalized)\n",
    "transform = None  \n",
    "\n",
    "mae_data_dir = '/home/vimarsh/Desktop/3-2/GSoC/ML4Sci/dataset/dataset/train'  # Should contain subfolders including \"no_sub\"\n",
    "target_class = 'no'  # Only use samples with no substructure for pretraining\n",
    "mae_dataset = FilteredNpyImageDataset(mae_data_dir, target_class=target_class, transform=transform)\n",
    "mae_loader = DataLoader(mae_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "# Initialize the MAE model (assuming images are single-channel)\n",
    "mae_model = MAE(in_channels=1, latent_dim=256)\n",
    "print(\"Starting MAE pretraining...\")\n",
    "mae_model = train_mae(mae_model, mae_loader, num_epochs=15, device=device, mask_ratio=0.30, patch_size=15)\n",
    "\n",
    "# Get the pretrained encoder for later use\n",
    "pretrained_encoder = mae_model.encoder\n",
    "\n",
    "\n",
    "# 6A task\n",
    "\n",
    "# Full dataset paths for training and validation\n",
    "train_dir = '/home/vimarsh/Desktop/3-2/GSoC/ML4Sci/dataset/dataset/train'\n",
    "val_dir   = '/home/vimarsh/Desktop/3-2/GSoC/ML4Sci/dataset/dataset/val'\n",
    "dataset_train = NpyImageDataset(train_dir, transform=transform)\n",
    "dataset_val   = NpyImageDataset(val_dir, transform=transform)\n",
    "train_loader_cls = DataLoader(dataset_train, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader_cls   = DataLoader(dataset_val, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# Build improved classification model using the pretrained encoder\n",
    "classifier_model = ClassificationModel(encoder=pretrained_encoder, latent_dim=256, num_classes=3)\n",
    "\n",
    "# Freeze the encoder for a few epochs to allow the new layers to adapt\n",
    "for param in classifier_model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"Starting classifier fine-tuning with frozen encoder...\")\n",
    "classifier_model = train_classifier(classifier_model, train_loader_cls, val_loader_cls, num_epochs=15, device=device)\n",
    "\n",
    "# Now unfreeze the encoder and continue training with a lower learning rate\n",
    "for param in classifier_model.encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(\"Fine-tuning the entire model...\")\n",
    "classifier_model = train_classifier(classifier_model, train_loader_cls, val_loader_cls, num_epochs=20, device=device)\n",
    "\n",
    "\n",
    "\n",
    "## 6B task\n",
    "\n",
    "# Directories for low-resolution (LR) and high-resolution (HR) images.\n",
    "lr_dir = '/home/vimarsh/Desktop/3-2/GSoC/ML4Sci/SuperRes_Dataset/Dataset/LR'\n",
    "hr_dir = '/home/vimarsh/Desktop/3-2/GSoC/ML4Sci/SuperRes_Dataset/Dataset/HR'\n",
    "sr_dataset = NpySuperResolutionDataset(lr_dir, hr_dir, transform_lr=transform, transform_hr=transform)\n",
    "sr_loader = DataLoader(sr_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "# Build improved super-resolution model using the pretrained encoder\n",
    "sr_model = SuperResolutionModel(encoder=pretrained_encoder, latent_dim=256, in_channels=1)\n",
    "print(\"Starting super-resolution fine-tuning...\")\n",
    "sr_model = train_super_resolution(sr_model, sr_loader, num_epochs=20, device=device)\n",
    "\n",
    "    \n",
    "#saving the final models\n",
    "torch.save(mae_model.state_dict(), 'mae_model.pth')\n",
    "torch.save(classifier_model.state_dict(), 'classifier_model.pth')\n",
    "torch.save(sr_model.state_dict(), 'superres_model.pth')\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
